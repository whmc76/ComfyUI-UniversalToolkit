"""
Load Video Frames Node for ComfyUI Universal Toolkit
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Load frames from video files or image sequences and intelligently sample frames
based on target frame count and sampling mode.

:copyright: (c) 2024 by May
:license: MIT, see LICENSE for more details.
"""

import os
import cv2
import torch
from typing import Optional, Tuple, List
from PIL import Image

from ..image.image_converters import pil2tensor


class Extract_Video_Frames_UTK:
    """
    ä»è§†é¢‘æ–‡ä»¶æˆ–å›¾ç‰‡åºåˆ—ä¸­æ™ºèƒ½æŠ½å–å¸§
    
    æ”¯æŒå¤šç§æŠ½å–æ¨¡å¼ï¼š
    - å¹³å‡æŠ½å–ï¼šå‡åŒ€åˆ†å¸ƒåœ¨æ•´ä¸ªè§†é¢‘/åºåˆ—ä¸­
    - å‰é¢è¾ƒå¤šï¼šå‰åŠéƒ¨åˆ†æŠ½å–æ›´å¤šå¸§
    - åé¢è¾ƒå¤šï¼šååŠéƒ¨åˆ†æŠ½å–æ›´å¤šå¸§
    - ä¸­é—´è¾ƒå¤šï¼šä¸­é—´éƒ¨åˆ†æŠ½å–æ›´å¤šå¸§
    - ä¸¤ç«¯è¾ƒå¤šï¼šå¼€å¤´å’Œç»“å°¾æŠ½å–æ›´å¤šå¸§
    """
    
    CATEGORY = "UniversalToolkit/Tools"
    RETURN_TYPES = ("IMAGE", "INT")
    RETURN_NAMES = ("images", "frames_count")
    FUNCTION = "load_frames"
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video_path": ("STRING", {
                    "default": "",
                    "tooltip": "è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒmp4, avi, movç­‰æ ¼å¼ï¼‰"
                }),
                "target_frames": ("INT", {
                    "default": 8,
                    "min": 1,
                    "max": 1000,
                    "step": 1,
                    "tooltip": "ç›®æ ‡æŠ½å–çš„å¸§æ•°"
                }),
                "mode": (["average", "front_heavy", "back_heavy", "middle_heavy", "ends_heavy"], {
                    "default": "average",
                    "tooltip": "Frame extraction mode"
                }),
            },
            "optional": {
                "images": ("IMAGE", {
                    "tooltip": "å›¾ç‰‡åºåˆ—è¾“å…¥ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¼˜å…ˆä½¿ç”¨å›¾ç‰‡åºåˆ—è€Œä¸æ˜¯è§†é¢‘ï¼‰"
                }),
            }
        }
    
    def calculate_frame_indices(self, total_frames: int, target_frames: int, mode: str) -> List[int]:
        """
        æ ¹æ®æ¨¡å¼å’Œç›®æ ‡å¸§æ•°è®¡ç®—è¦æŠ½å–çš„å¸§ç´¢å¼•
        
        Args:
            total_frames: æ€»å¸§æ•°
            target_frames: ç›®æ ‡æŠ½å–å¸§æ•°
            mode: æŠ½å–æ¨¡å¼
            
        Returns:
            å¸§ç´¢å¼•åˆ—è¡¨
        """
        if total_frames <= 0:
            return []
        
        if target_frames >= total_frames:
            # å¦‚æœç›®æ ‡å¸§æ•°å¤§äºç­‰äºæ€»å¸§æ•°ï¼Œè¿”å›æ‰€æœ‰å¸§
            return list(range(total_frames))
        
        indices = []
        
        if mode == "average":
            # å‡åŒ€åˆ†å¸ƒ
            step = total_frames / target_frames
            indices = [int(i * step) for i in range(target_frames)]
            # ç¡®ä¿æœ€åä¸€ä¸ªç´¢å¼•ä¸è¶…è¿‡æ€»å¸§æ•°
            indices[-1] = min(indices[-1], total_frames - 1)
            
        elif mode == "front_heavy":
            # å‰åŠéƒ¨åˆ†æŠ½å–60%ï¼ŒååŠéƒ¨åˆ†æŠ½å–40%
            front_count = int(target_frames * 0.6)
            back_count = target_frames - front_count
            
            # å‰åŠéƒ¨åˆ†å‡åŒ€æŠ½å–
            if front_count > 0:
                front_step = (total_frames // 2) / front_count
                front_indices = [int(i * front_step) for i in range(front_count)]
            else:
                front_indices = []
            
            # ååŠéƒ¨åˆ†å‡åŒ€æŠ½å–
            if back_count > 0:
                back_start = total_frames // 2
                back_step = (total_frames - back_start) / back_count
                back_indices = [back_start + int(i * back_step) for i in range(back_count)]
            else:
                back_indices = []
            
            indices = front_indices + back_indices
            
        elif mode == "back_heavy":
            # å‰åŠéƒ¨åˆ†æŠ½å–40%ï¼ŒååŠéƒ¨åˆ†æŠ½å–60%
            front_count = int(target_frames * 0.4)
            back_count = target_frames - front_count
            
            # å‰åŠéƒ¨åˆ†å‡åŒ€æŠ½å–
            if front_count > 0:
                front_step = (total_frames // 2) / front_count
                front_indices = [int(i * front_step) for i in range(front_count)]
            else:
                front_indices = []
            
            # ååŠéƒ¨åˆ†å‡åŒ€æŠ½å–
            if back_count > 0:
                back_start = total_frames // 2
                back_step = (total_frames - back_start) / back_count
                back_indices = [back_start + int(i * back_step) for i in range(back_count)]
            else:
                back_indices = []
            
            indices = front_indices + back_indices
            
        elif mode == "middle_heavy":
            # å¼€å¤´20%ï¼Œä¸­é—´60%ï¼Œç»“å°¾20%
            start_count = int(target_frames * 0.2)
            middle_count = int(target_frames * 0.6)
            end_count = target_frames - start_count - middle_count
            
            # å¼€å¤´éƒ¨åˆ†
            if start_count > 0:
                start_step = (total_frames // 4) / max(start_count, 1)
                start_indices = [int(i * start_step) for i in range(start_count)]
            else:
                start_indices = []
            
            # ä¸­é—´éƒ¨åˆ†
            if middle_count > 0:
                middle_start = total_frames // 4
                middle_end = total_frames * 3 // 4
                middle_step = (middle_end - middle_start) / max(middle_count, 1)
                middle_indices = [middle_start + int(i * middle_step) for i in range(middle_count)]
            else:
                middle_indices = []
            
            # ç»“å°¾éƒ¨åˆ†
            if end_count > 0:
                end_start = total_frames * 3 // 4
                end_step = (total_frames - end_start) / max(end_count, 1)
                end_indices = [end_start + int(i * end_step) for i in range(end_count)]
            else:
                end_indices = []
            
            indices = start_indices + middle_indices + end_indices
            
        elif mode == "ends_heavy":
            # å¼€å¤´40%ï¼Œä¸­é—´20%ï¼Œç»“å°¾40%
            start_count = int(target_frames * 0.4)
            middle_count = int(target_frames * 0.2)
            end_count = target_frames - start_count - middle_count
            
            # å¼€å¤´éƒ¨åˆ†
            if start_count > 0:
                start_step = (total_frames // 3) / max(start_count, 1)
                start_indices = [int(i * start_step) for i in range(start_count)]
            else:
                start_indices = []
            
            # ä¸­é—´éƒ¨åˆ†
            if middle_count > 0:
                middle_start = total_frames // 3
                middle_end = total_frames * 2 // 3
                middle_step = (middle_end - middle_start) / max(middle_count, 1)
                middle_indices = [middle_start + int(i * middle_step) for i in range(middle_count)]
            else:
                middle_indices = []
            
            # ç»“å°¾éƒ¨åˆ†
            if end_count > 0:
                end_start = total_frames * 2 // 3
                end_step = (total_frames - end_start) / max(end_count, 1)
                end_indices = [end_start + int(i * end_step) for i in range(end_count)]
            else:
                end_indices = []
            
            indices = start_indices + middle_indices + end_indices
        
        # å»é‡å¹¶æ’åº
        indices = sorted(list(set(indices)))
        # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        indices = [idx for idx in indices if 0 <= idx < total_frames]
        
        # å¦‚æœå»é‡åæ•°é‡ä¸è¶³ï¼Œè¡¥å……å¸§
        while len(indices) < target_frames and len(indices) < total_frames:
            # æ‰¾åˆ°æœ€å¤§çš„é—´éš”å¹¶è¡¥å……
            if len(indices) == 0:
                indices.append(0)
            elif len(indices) == 1:
                if indices[0] < total_frames - 1:
                    indices.append(total_frames - 1)
                else:
                    break
            else:
                max_gap = 0
                insert_pos = 0
                for i in range(len(indices) - 1):
                    gap = indices[i + 1] - indices[i]
                    if gap > max_gap:
                        max_gap = gap
                        insert_pos = i + 1
                        insert_value = (indices[i] + indices[i + 1]) // 2
                
                if max_gap > 1:
                    indices.insert(insert_pos, insert_value)
                    indices.sort()
                else:
                    # å¦‚æœæ²¡æœ‰å¤§é—´éš”ï¼Œåœ¨ä¸¤ç«¯è¡¥å……
                    if indices[0] > 0:
                        indices.insert(0, indices[0] - 1)
                    elif indices[-1] < total_frames - 1:
                        indices.append(min(indices[-1] + 1, total_frames - 1))
                    else:
                        break
        
        return indices[:target_frames]
    
    def load_video_frames(self, video_path: str, indices: List[int]) -> List[torch.Tensor]:
        """
        ä»è§†é¢‘æ–‡ä»¶ä¸­åŠ è½½æŒ‡å®šç´¢å¼•çš„å¸§
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            indices: è¦åŠ è½½çš„å¸§ç´¢å¼•åˆ—è¡¨
            
        Returns:
            å¸§å¼ é‡åˆ—è¡¨
        """
        # è·¯å¾„é¢„å¤„ç†
        video_path = video_path.strip().strip('"').strip("'")
        video_path = video_path.replace("\\", "/")
        
        if not os.path.isfile(video_path):
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise RuntimeError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        # ä¸ºäº†ä¿æŒåŸå§‹é¡ºåºï¼Œå…ˆæŒ‰ç´¢å¼•é¡ºåºè¯»å–å¹¶å­˜å‚¨åˆ°å­—å…¸ä¸­
        frame_dict = {}
        sorted_indices = sorted(set(indices))  # å»é‡å¹¶æ’åºä»¥æé«˜æ•ˆç‡
        
        for target_idx in sorted_indices:
            # è·³è½¬åˆ°ç›®æ ‡å¸§
            cap.set(cv2.CAP_PROP_POS_FRAMES, target_idx)
            
            ret, frame = cap.read()
            if not ret:
                print(f"è­¦å‘Š: æ— æ³•è¯»å–ç¬¬ {target_idx} å¸§")
                continue
            
            # è½¬æ¢BGRåˆ°RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            # è½¬æ¢ä¸ºPILå›¾åƒ
            pil_image = Image.fromarray(frame_rgb)
            # è½¬æ¢ä¸ºtensor
            tensor = pil2tensor(pil_image)
            frame_dict[target_idx] = tensor
        
        cap.release()
        
        if len(frame_dict) == 0:
            raise RuntimeError("æœªèƒ½ä»è§†é¢‘ä¸­åŠ è½½ä»»ä½•å¸§")
        
        # æŒ‰ç…§åŸå§‹indicesé¡ºåºè¿”å›å¸§
        frames = [frame_dict[idx] for idx in indices if idx in frame_dict]
        
        return frames
    
    def load_image_sequence_frames(self, images: torch.Tensor, indices: List[int]) -> List[torch.Tensor]:
        """
        ä»å›¾ç‰‡åºåˆ—ä¸­æå–æŒ‡å®šç´¢å¼•çš„å¸§
        
        Args:
            images: å›¾ç‰‡æ‰¹æ¬¡å¼ é‡ [batch, height, width, channels]
            indices: è¦æå–çš„å¸§ç´¢å¼•åˆ—è¡¨
            
        Returns:
            å¸§å¼ é‡åˆ—è¡¨
        """
        frames = []
        for idx in indices:
            if 0 <= idx < len(images):
                frames.append(images[idx])
            else:
                print(f"è­¦å‘Š: ç´¢å¼• {idx} è¶…å‡ºå›¾ç‰‡åºåˆ—èŒƒå›´ [0, {len(images)-1}]")
        
        return frames
    
    def load_frames(
        self,
        video_path: str,
        target_frames: int,
        mode: str,
        images: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor]:
        """
        åŠ è½½å¹¶æŠ½å–å¸§
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            target_frames: ç›®æ ‡å¸§æ•°
            mode: æŠ½å–æ¨¡å¼
            images: å¯é€‰çš„å›¾ç‰‡åºåˆ—è¾“å…¥
            
        Returns:
            æŠ½å–çš„å¸§æ‰¹æ¬¡å¼ é‡
        """
        # ä¼˜å…ˆä½¿ç”¨å›¾ç‰‡åºåˆ—è¾“å…¥
        if images is not None:
            total_frames = len(images)
            print(f"ğŸ“¸ ä»å›¾ç‰‡åºåˆ—ä¸­æŠ½å–å¸§: æ€»å¸§æ•°={total_frames}, ç›®æ ‡å¸§æ•°={target_frames}, æ¨¡å¼={mode}")
            
            indices = self.calculate_frame_indices(total_frames, target_frames, mode)
            print(f"ğŸ“Š è®¡ç®—å¾—åˆ°çš„å¸§ç´¢å¼•: {indices}")
            
            # æŒ‰ç…§indicesçš„é¡ºåºæå–å¸§ï¼ˆä¿æŒè®¡ç®—å‡ºçš„é¡ºåºï¼‰
            frame_tensors = self.load_image_sequence_frames(images, indices)
            
        elif video_path and video_path.strip():
            # ä½¿ç”¨è§†é¢‘æ–‡ä»¶
            video_path = video_path.strip().strip('"').strip("'")
            
            # å…ˆè·å–è§†é¢‘æ€»å¸§æ•°
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise RuntimeError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            cap.release()
            
            print(f"ğŸ¬ ä»è§†é¢‘ä¸­æŠ½å–å¸§: æ–‡ä»¶={video_path}, æ€»å¸§æ•°={total_frames}, FPS={fps:.2f}, ç›®æ ‡å¸§æ•°={target_frames}, æ¨¡å¼={mode}")
            
            indices = self.calculate_frame_indices(total_frames, target_frames, mode)
            print(f"ğŸ“Š è®¡ç®—å¾—åˆ°çš„å¸§ç´¢å¼•: {indices}")
            
            frame_tensors = self.load_video_frames(video_path, indices)
            
        else:
            raise ValueError("å¿…é¡»æä¾›è§†é¢‘è·¯å¾„æˆ–å›¾ç‰‡åºåˆ—è¾“å…¥")
        
        if len(frame_tensors) == 0:
            raise RuntimeError("æœªèƒ½åŠ è½½ä»»ä½•å¸§")
        
        # å°†æ‰€æœ‰å¸§å †å æˆæ‰¹æ¬¡
        batch_tensor = torch.stack(frame_tensors, dim=0)
        frames_count = len(frame_tensors)
        
        print(f"âœ… æˆåŠŸåŠ è½½ {frames_count} å¸§ï¼Œè¾“å‡ºå½¢çŠ¶: {batch_tensor.shape}")
        
        return (batch_tensor, frames_count)


# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "Extract_Video_Frames_UTK": Extract_Video_Frames_UTK
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Extract_Video_Frames_UTK": "Extract Video Frames (UTK)"
}

